{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*1.  What is Boosting in Machine Learning?*\n",
        "\n",
        "Boosting is an ensemble machine learning technique that aims to improve the performance of weak learners (typically simple models like decision trees) by combining them into a single strong learner. The core idea is to train models sequentially, where each new model focuses on correcting the errors made by the previous ones.\n",
        "\n",
        "*2.  How does Boosting differ from Bagging ?*\n",
        "\n",
        "Boosting and Bagging are both ensemble learning techniques used to improve the performance of machine learning models by combining multiple learners. However, they differ significantly in how they build and combine these learners.\n",
        "\n",
        "Key Differences Between Boosting and Bagging\n",
        "\n",
        "| Feature                 | **Bagging**                                                                     | **Boosting**                                                            |\n",
        "| ----------------------- | ------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |\n",
        "| **Goal**                | Reduce variance (prevent overfitting)                                           | Reduce bias (improve model accuracy)                                    |\n",
        "| **Learning Process**    | Learners are trained **independently** in **parallel**                          | Learners are trained **sequentially**, each depending on the previous   |\n",
        "| **Data Sampling**       | Uses **bootstrap sampling** (random samples with replacement)                   | No bootstrap sampling; each model focuses on errors from previous model |\n",
        "| **Model Focus**         | All models get equal weight                                                     | Later models focus more on previously misclassified examples            |\n",
        "| **Combining Models**    | Typically uses **averaging** (regression) or **majority vote** (classification) | Uses **weighted sum** of predictions                                    |\n",
        "| **Common Algorithms**   | Random Forest (bagging of decision trees)                                       | AdaBoost, Gradient Boosting, XGBoost, LightGBM                          |\n",
        "| **Risk of Overfitting** | Lower (good for high variance models)                                           | Higher (can overfit if not tuned properly)                              |\n",
        "\n",
        "\n",
        "*3.  What is the key idea behind AdaBoost?*\n",
        "\n",
        "Key Idea Behind AdaBoost (Adaptive Boosting):\n",
        "AdaBoost improves the performance of weak learners by focusing on the mistakes made by previous models and adjusting the weights of the training data accordingly. Each new model tries to correct the errors of the combined previous models.\n",
        "\n",
        "Core Concepts of AdaBoost:\n",
        "Weak Learners:\n",
        "\n",
        "Typically shallow decision trees (stumps).\n",
        "\n",
        "Each performs only slightly better than random guessing.\n",
        "\n",
        "Weighted Training:\n",
        "\n",
        "Initially, all data points are equally weighted.\n",
        "\n",
        "After each round, weights of misclassified points increase, so the next model focuses more on them.\n",
        "\n",
        "Model Combination:\n",
        "\n",
        "After all learners are trained, their predictions are combined using a weighted vote, where more accurate learners have more influence.\n",
        "\n",
        "Adaptivity:\n",
        "\n",
        "The name “Adaptive Boosting” comes from the way the algorithm adapts to the mistakes of earlier learners.\n",
        "\n",
        "*4.  Explain the working of AdaBoost with an example?*\n",
        "\n",
        "AdaBoost learns from mistakes and builds a strong classifier by combining weak ones that focus on different subsets of the data.\n",
        "\n",
        "AdaBoost Step-by-Step Example\n",
        "✅ Problem: Simple binary classification\n",
        "Imagine we have the following dataset:\n",
        "\n",
        "Instance\tFeature (X)\tLabel (Y)\n",
        "1\tRed\t+1\n",
        "2\tRed\t+1\n",
        "3\tBlue\t-1\n",
        "4\tBlue\t-1\n",
        "5\tBlue\t+1\n",
        "\n",
        "We’ll train weak learners (decision stumps) using AdaBoost for 3 rounds.\n",
        "\n",
        "🧩 Step 1: Initialize Weights\n",
        "Since we have 5 instances, each gets an equal weight:\n",
        "\n",
        "𝑤\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "5\n",
        "=\n",
        "0.2\n",
        "w\n",
        "i\n",
        "​\n",
        " =\n",
        "5\n",
        "1\n",
        "​\n",
        " =0.2\n",
        "🧩 Step 2: Train First Weak Learner\n",
        "Let’s say the first decision stump is:\n",
        "\n",
        "mathematica\n",
        "Copy\n",
        "Edit\n",
        "If color == Red → Predict +1\n",
        "Else → Predict -1\n",
        "Predictions:\n",
        "\n",
        "Instance\tX\tTrue Y\tPredicted Y\tCorrect?\n",
        "1\tRed\t+1\t+1\t✅\n",
        "2\tRed\t+1\t+1\t✅\n",
        "3\tBlue\t-1\t-1\t✅\n",
        "4\tBlue\t-1\t-1\t✅\n",
        "5\tBlue\t+1\t-1\t❌\n",
        "\n",
        "Total error (ε₁) = weight of misclassified = 0.2\n",
        "\n",
        "Model weight (α₁):\n",
        "\n",
        "𝛼\n",
        "1\n",
        "=\n",
        "1\n",
        "2\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝜖\n",
        "1\n",
        "𝜖\n",
        "1\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "0.8\n",
        "0.2\n",
        ")\n",
        "=\n",
        "0.5\n",
        "⋅\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "4\n",
        ")\n",
        "≈\n",
        "0.693\n",
        "α\n",
        "1\n",
        "​\n",
        " =\n",
        "2\n",
        "1\n",
        "​\n",
        " ln(\n",
        "ϵ\n",
        "1\n",
        "​\n",
        "\n",
        "1−ϵ\n",
        "1\n",
        "​\n",
        "\n",
        "​\n",
        " )=\n",
        "2\n",
        "1\n",
        "​\n",
        " ln(\n",
        "0.2\n",
        "0.8\n",
        "​\n",
        " )=0.5⋅ln(4)≈0.693\n",
        "🧩 Step 3: Update Weights\n",
        "Misclassified point (instance 5) → increase its weight.\n",
        "\n",
        "Correctly classified points → decrease their weights.\n",
        "\n",
        "Update rule:\n",
        "\n",
        "𝑤\n",
        "𝑖\n",
        "𝑛\n",
        "𝑒\n",
        "𝑤\n",
        "=\n",
        "𝑤\n",
        "𝑖\n",
        "⋅\n",
        "𝑒\n",
        "−\n",
        "𝛼\n",
        "𝑦\n",
        "𝑖\n",
        "ℎ\n",
        "𝑖\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "w\n",
        "i\n",
        "new\n",
        "​\n",
        " =w\n",
        "i\n",
        "​\n",
        " ⋅e\n",
        "−αy\n",
        "i\n",
        "​\n",
        " h\n",
        "i\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " )\n",
        "\n",
        "Normalize all weights so they sum to 1.\n",
        "\n",
        "Result: instance 5 gets higher weight in the next round.\n",
        "\n",
        "🧩 Step 4: Train Second Weak Learner\n",
        "The next learner focuses more on instance 5. Suppose it predicts +1 for Blue this time.\n",
        "\n",
        "Predictions:\n",
        "\n",
        "This time it may classify instance 5 correctly and another one incorrectly, based on the new weights.\n",
        "\n",
        "Repeat steps to compute new error (ε₂), model weight (α₂), and update weights again.\n",
        "\n",
        "🧩 Step 5: Final Model\n",
        "After T rounds, the final prediction is:\n",
        "\n",
        "𝐻\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "sign\n",
        "(\n",
        "∑\n",
        "𝑡\n",
        "=\n",
        "1\n",
        "𝑇\n",
        "𝛼\n",
        "𝑡\n",
        "⋅\n",
        "ℎ\n",
        "𝑡\n",
        "(\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "H(x)=sign(\n",
        "t=1\n",
        "∑\n",
        "T\n",
        "​\n",
        " α\n",
        "t\n",
        "​\n",
        " ⋅h\n",
        "t\n",
        "​\n",
        " (x))\n",
        "Each weak learner’s vote is weighted by its accuracy (α).\n",
        "\n",
        "More accurate learners have more influence on the final result.\n",
        "\n",
        "*5.  What is Gradient Boosting, and how is it different from AdaBoost?*\n",
        "\n",
        "Gradient Boosting is an ensemble machine learning technique that builds a strong model by sequentially adding weak learners (usually decision trees), where each new learner is trained to minimize the errors (residuals) of the current model using gradient descent.\n",
        "\n",
        "Instead of adjusting sample weights (as in AdaBoost), Gradient Boosting fits each new model to the negative gradient (i.e., residual errors) of the loss function with respect to the current model’s predictions.\n",
        "\n",
        "Gradient Boosting vs. AdaBoost: Key Differences\n",
        "\n",
        "\n",
        "| Feature                 | **AdaBoost**                                | **Gradient Boosting**                         |\n",
        "| ----------------------- | ------------------------------------------- | --------------------------------------------- |\n",
        "| **Error Handling**      | Adjusts weights of misclassified samples    | Fits the new model to the residual errors     |\n",
        "| **Loss Function**       | Typically exponential loss (binary)         | Any differentiable loss (e.g., MSE, log loss) |\n",
        "| **Optimization Method** | Heuristic (sample reweighting)              | Gradient descent in function space            |\n",
        "| **Flexibility**         | Mostly for classification                   | Works well for regression & classification    |\n",
        "| **Model Focus**         | Learner focuses on hard-to-classify samples | Learner fits to gradient of loss (residuals)  |\n",
        "| **Interpretability**    | Slightly more intuitive                     | More flexible but harder to interpret         |\n",
        "| **Performance**         | Fast and effective for simple tasks         | Often better for complex tasks (with tuning)  |\n",
        "\n",
        "\n",
        "*6.  What is the loss function in Gradient Boosting?*\n",
        "\n",
        "In Gradient Boosting, the loss function quantifies how well the model is performing. The algorithm uses this loss function to compute gradients (i.e., errors) and guides the model to reduce those errors in successive steps — just like in regular gradient descent.\n",
        "\n",
        "Common Loss Functions in Gradient Boosting\n",
        "1. For Regression Tasks:\n",
        "Loss Function\tFormula\tGradient (used as residual)\n",
        "Squared Error (MSE)\n",
        "𝐿\n",
        "(\n",
        "𝑦\n",
        ",\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "(\n",
        "𝑦\n",
        "−\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "2\n",
        "L(y,\n",
        "y\n",
        "^\n",
        "​\n",
        " )=\n",
        "2\n",
        "1\n",
        "​\n",
        " (y−\n",
        "y\n",
        "^\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "−\n",
        "∂\n",
        "𝐿\n",
        "∂\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "𝑦\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "−\n",
        "∂\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "∂L\n",
        "​\n",
        " =y−\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "Absolute Error (MAE)\t( L(y, \\hat{y}) =\ty - \\hat{y}\n",
        "Huber Loss\tCombines MSE and MAE for robustness\tVaries depending on residual size\n",
        "\n",
        "2. For Classification Tasks:\n",
        "Problem Type\tLoss Function\tNotes\n",
        "Binary Classification\tLog Loss (Binary Cross-Entropy):\n",
        "𝐿\n",
        "(\n",
        "𝑦\n",
        ",\n",
        "𝑝\n",
        "^\n",
        ")\n",
        "=\n",
        "−\n",
        "[\n",
        "𝑦\n",
        "log\n",
        "⁡\n",
        "𝑝\n",
        "^\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "^\n",
        ")\n",
        "]\n",
        "L(y,\n",
        "p\n",
        "^\n",
        "​\n",
        " )=−[ylog\n",
        "p\n",
        "^\n",
        "​\n",
        " +(1−y)log(1−\n",
        "p\n",
        "^\n",
        "​\n",
        " )]\tMost common; smooth, differentiable\n",
        "Multiclass Classification\tMulticlass Log Loss\tUses softmax and cross-entropy for multiple classes\n",
        "\n",
        "🧠 Intuition:\n",
        "Loss function tells you how wrong your model is.\n",
        "\n",
        "Gradient of that loss tells you how to fix it.\n",
        "\n",
        "Each weak learner is trained to predict the gradient (the residual).\n",
        "\n",
        "🔄 Example (Squared Error for Regression):\n",
        "Suppose the current prediction is\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "True value is\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "Loss:\n",
        "\n",
        "𝐿\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "2\n",
        "L(y\n",
        "i\n",
        "​\n",
        " ,\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )=\n",
        "2\n",
        "1\n",
        "​\n",
        " (y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Gradient (residual):\n",
        "\n",
        "𝑟\n",
        "𝑖\n",
        "=\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        "r\n",
        "i\n",
        "​\n",
        " =y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        "\n",
        "The weak learner is trained to fit these residuals, then the model is updated.\n",
        "\n",
        "🛠 Custom Loss Functions\n",
        "Advanced gradient boosting frameworks like XGBoost, LightGBM, and CatBoost allow you to:\n",
        "\n",
        "Use custom loss functions (as long as they’re differentiable).\n",
        "\n",
        "Control training with regularization and other constraints.\n",
        "\n",
        "*7.  How does XGBoost improve over traditional Gradient Boosting?*\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) is a powerful and widely used implementation of gradient boosting. It significantly improves over traditional Gradient Boosting (like scikit-learn's GradientBoostingClassifier) in speed, accuracy, and efficiency, thanks to several technical innovations.\n",
        "\n",
        "Key Improvements of XGBoost over Traditional Gradient Boosting\n",
        "\n",
        "| Feature                       | Traditional Gradient Boosting    | XGBoost                                                                                        |\n",
        "| ----------------------------- | -------------------------------- | ---------------------------------------------------------------------------------------------- |\n",
        "| **Optimization Method**       | Uses plain gradient descent      | Uses **second-order gradient descent** (uses both gradient and Hessian for better convergence) |\n",
        "| **Regularization**            | Usually not included             | Includes **L1 & L2 regularization** to reduce overfitting                                      |\n",
        "| **Handling Missing Data**     | Often needs preprocessing        | **Built-in handling** of missing values                                                        |\n",
        "| **Tree Construction**         | Level-wise                       | **Depth-wise (greedy)** and more efficient                                                     |\n",
        "| **Parallelization**           | Typically not parallelized       | **Parallel processing** of trees for faster training                                           |\n",
        "| **Shrinkage (Learning Rate)** | Manual                           | Supported with advanced tuning                                                                 |\n",
        "| **Feature Importance**        | Basic support                    | Detailed **gain/cover/frequency-based** importance metrics                                     |\n",
        "| **Early Stopping**            | Limited                          | Fully supported                                                                                |\n",
        "| **Cross-validation**          | External (manual with `sklearn`) | **Built-in CV** support                                                                        |\n",
        "\n",
        "\n",
        "*8.  What is the difference between XGBoost and CatBoost?*\n",
        "\n",
        " XGBoost and CatBoost are both gradient boosting frameworks designed for performance and accuracy — but they differ in several important ways, especially in how they handle categorical data, efficiency, and ease of use.\n",
        "\n",
        " XGBoost vs CatBoost – Key Differences\n",
        "\n",
        " | Feature                  | **XGBoost**                                                | **CatBoost**                                                            |\n",
        "| ------------------------ | ---------------------------------------------------------- | ----------------------------------------------------------------------- |\n",
        "| **Developer**            | DMLC (Distributed ML Community)                            | Yandex (Russian tech company)                                           |\n",
        "| **Categorical Features** | Must be manually encoded (e.g., one-hot or label encoding) | **Natively handles** categorical variables with automatic encoding      |\n",
        "| **Training Speed**       | Fast, but may need tuning                                  | Often faster **out-of-the-box** for categorical data                    |\n",
        "| **Default Accuracy**     | High, needs preprocessing                                  | **Very high**, minimal preprocessing needed                             |\n",
        "| **Missing Values**       | Handled well (auto-learn split directions)                 | Also handled automatically                                              |\n",
        "| **Overfitting Control**  | L1/L2 regularization, tree pruning                         | Similar regularization + **ordered boosting** for better generalization |\n",
        "| **Ease of Use**          | Requires manual preprocessing                              | Very **user-friendly**, less data prep                                  |\n",
        "| **Support for Text**     | Not natively                                               | **Supports text** features (tokenization, embeddings)                   |\n",
        "| **GPU Support**          | Yes (with proper config)                                   | Yes, **optimized for GPU** training                                     |\n",
        "| **Interpretability**     | SHAP, feature importance                                   | SHAP, built-in visualization tools                                      |\n",
        "\n",
        "\n",
        "*9. What are some real-world applications of Boosting techniques?*\n",
        "\n",
        "Boosting techniques — especially Gradient Boosting, XGBoost, LightGBM, and CatBoost — are widely used in real-world machine learning due to their high predictive accuracy, robustness, and versatility. Here are some common and impactful real-world applications:\n",
        "\n",
        "Real-World Applications of Boosting\n",
        "1. 🏦 Finance\n",
        "Credit scoring: Predicting creditworthiness of loan applicants.\n",
        "\n",
        "Fraud detection: Identifying anomalous transactions.\n",
        "\n",
        "Stock price prediction: Modeling time-series behavior for short-term forecasting.\n",
        "\n",
        "✅ Why Boosting?\n",
        "Handles complex, non-linear relationships and class imbalance very well (e.g., fraud detection).\n",
        "\n",
        "2. 🏥 Healthcare\n",
        "Disease diagnosis: Predicting diseases based on symptoms, lab results, or imaging data.\n",
        "\n",
        "Patient risk prediction: Estimating the likelihood of readmission or complications.\n",
        "\n",
        "Drug discovery: Modeling chemical properties and biological activity.\n",
        "\n",
        "✅ Why Boosting?\n",
        "Delivers high accuracy and works well with structured medical records and lab data.\n",
        "\n",
        "3. 🛒 E-Commerce & Retail\n",
        "Customer churn prediction: Identifying customers likely to stop using a service.\n",
        "\n",
        "Product recommendation systems: Personalizing shopping experience.\n",
        "\n",
        "Demand forecasting: Predicting future sales based on historical patterns.\n",
        "\n",
        "✅ Why Boosting?\n",
        "Excels with tabular data like customer history, product details, and transaction logs.\n",
        "\n",
        "4. 🎯 Marketing & Advertising\n",
        "Click-through rate (CTR) prediction: Optimizing which ads to show.\n",
        "\n",
        "Lead scoring: Identifying which prospects are most likely to convert.\n",
        "\n",
        "Campaign optimization: Predicting outcomes of different ad strategies.\n",
        "\n",
        "✅ Why Boosting?\n",
        "CatBoost and LightGBM are especially strong for categorical-heavy ad and user data.\n",
        "\n",
        "5. 🎓 Education\n",
        "Student performance prediction: Identifying students at risk of failing.\n",
        "\n",
        "Personalized learning: Adapting material based on predicted learning pace.\n",
        "\n",
        "6. 🏢 Human Resources\n",
        "Resume screening: Predicting job fit or success probability.\n",
        "\n",
        "Attrition prediction: Forecasting which employees are likely to leave.\n",
        "\n",
        "7. 🚗 Transportation & Logistics\n",
        "ETA prediction: Estimating delivery or arrival times.\n",
        "\n",
        "Route optimization: Predicting traffic congestion or delivery delays.\n",
        "\n",
        "8. 📱 Telecommunications\n",
        "Network failure prediction: Proactively identifying infrastructure issues.\n",
        "\n",
        "Customer segmentation: Understanding usage behavior and tailoring plans.\n",
        "\n",
        "9. 🏛️ Government & Public Policy\n",
        "Tax fraud detection\n",
        "\n",
        "Predictive policing\n",
        "\n",
        "Welfare eligibility models\n",
        "\n",
        "*10.  How does regularization help in XGBoost?*\n",
        "\n",
        "Regularization in XGBoost plays a crucial role in improving generalization and reducing the risk of overfitting — especially when working with complex models or noisy data.\n",
        "\n",
        "Regularization adds a penalty to the model's complexity in the objective function, discouraging overly complex models (e.g., very deep trees or high weights). This leads to simpler, more generalizable models.\n",
        "\n",
        "XGBoost’s objective function is:\n",
        "\n",
        "Obj\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝐿\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "+\n",
        "∑\n",
        "𝑘\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "Ω\n",
        "(\n",
        "𝑓\n",
        "𝑘\n",
        ")\n",
        "Obj=\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " L(y\n",
        "i\n",
        "​\n",
        " ,\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )+\n",
        "k=1\n",
        "∑\n",
        "K\n",
        "​\n",
        " Ω(f\n",
        "k\n",
        "​\n",
        " )\n",
        "Where:\n",
        "\n",
        "𝐿\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "L(y\n",
        "i\n",
        "​\n",
        " ,\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " ): loss function (e.g., log loss, MSE)\n",
        "\n",
        "Ω\n",
        "(\n",
        "𝑓\n",
        "𝑘\n",
        ")\n",
        "Ω(f\n",
        "k\n",
        "​\n",
        " ): regularization term for tree\n",
        "𝑓\n",
        "𝑘\n",
        "f\n",
        "k\n",
        "​\n",
        "\n",
        "Regularization Helps :\n",
        "\n",
        "1.  Reduces Overfitting\n",
        "Penalizing large trees or large weights prevents the model from fitting noise in the data.\n",
        "\n",
        "2.  Improves Generalization\n",
        "Encourages simpler models that perform better on unseen data.\n",
        "\n",
        "3.  Controls Model Complexity\n",
        "Parameters like max_depth, gamma, lambda, and alpha explicitly regulate how complex trees can grow.\n",
        "\n",
        "4.  Encourages Sparsity\n",
        "L1 regularization (alpha) can zero out unnecessary leaf weights, leading to sparse models.\n",
        "\n",
        "*11.  What are some hyperparameters to tune in Gradient Boosting models?*\n",
        "\n",
        "Tuning hyperparameters in Gradient Boosting models is key to getting the best performance while avoiding overfitting or underfitting. Here are some of the most important hyperparameters to tune, applicable across popular implementations like scikit-learn’s GradientBoosting, XGBoost, LightGBM, and CatBoost (though names may vary slightly).\n",
        "\n",
        " Key Hyperparameters to Tune in Gradient Boosting Models:\n",
        "\n",
        " | Hyperparameter                               | Description                                                                                                                     | Effect on Model                                                                       |\n",
        "| -------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |\n",
        "| **n\\_estimators**                            | Number of boosting rounds (trees)                                                                                               | More trees can improve performance but risk overfitting                               |\n",
        "| **learning\\_rate (eta)**                     | Step size shrinkage applied to each tree                                                                                        | Smaller values require more trees; controls overfitting                               |\n",
        "| **max\\_depth**                               | Maximum depth of each tree                                                                                                      | Controls tree complexity; deeper trees fit more complex patterns but risk overfitting |\n",
        "| **min\\_samples\\_split / min\\_child\\_weight** | Minimum samples required to split an internal node (scikit-learn) / Minimum sum of instance weights needed in a child (XGBoost) | Prevents splitting on noisy data; higher values make trees more conservative          |\n",
        "| **min\\_samples\\_leaf**                       | Minimum samples required to be at a leaf node                                                                                   | Prevents leaves with very few samples                                                 |\n",
        "| **subsample**                                | Fraction of samples used for fitting each tree                                                                                  | Introduces randomness; helps reduce overfitting                                       |\n",
        "| **colsample\\_bytree / max\\_features**        | Fraction of features used for each tree                                                                                         | Random feature selection; helps reduce overfitting                                    |\n",
        "| **gamma / min\\_split\\_loss**                 | Minimum loss reduction required to make a split (XGBoost)                                                                       | Controls tree pruning; larger values make splitting stricter                          |\n",
        "| **reg\\_alpha**                               | L1 regularization on leaf weights (XGBoost, LightGBM)                                                                           | Encourages sparsity; helps with feature selection                                     |\n",
        "| **reg\\_lambda**                              | L2 regularization on leaf weights (XGBoost, LightGBM)                                                                           | Smooths leaf weights; helps prevent overfitting                                       |\n",
        "\n",
        "*12.  What is the concept of Feature Importance in Boosting?*\n",
        "\n",
        "Feature Importance in boosting models helps you understand which input features contribute the most to the model’s predictions. It’s a key tool for interpreting complex models like Gradient Boosting, XGBoost, LightGBM, and CatBoost.\n",
        "\n",
        "Feature importance measures how valuable each feature is for making accurate predictions in the model. It answers questions like:\n",
        "\n",
        "Which features influence the outcome most?\n",
        "\n",
        "Which features can be safely ignored or removed?\n",
        "\n",
        "How to better understand model decisions?\n",
        "\n",
        "Feature Importance in Popular Boosting Libraries:\n",
        "\n",
        "| Library                      | How to Get Feature Importance                                                                       |\n",
        "| ---------------------------- | --------------------------------------------------------------------------------------------------- |\n",
        "| **XGBoost**                  | `model.get_score(importance_type='gain')` or `'weight'` or `'cover'`                                |\n",
        "| **LightGBM**                 | `model.feature_importance(importance_type='gain')` or `'split'`                                     |\n",
        "| **CatBoost**                 | `model.get_feature_importance()` with types like `'PredictionValuesChange'`, `'LossFunctionChange'` |\n",
        "| **sklearn GradientBoosting** | `model.feature_importances_` (based on mean decrease impurity)                                      |\n",
        "\n",
        "\n",
        "*13.  Why is CatBoost efficient for categorical data?*\n",
        "\n",
        "CatBoost is specifically designed to handle categorical features efficiently and effectively, which sets it apart from many other boosting algorithms like XGBoost or LightGBM that require manual preprocessing of categorical data.\n",
        "\n",
        " CatBoost Efficient for Categorical Data :\n",
        "\n",
        "1. Native Categorical Feature Support\n",
        "CatBoost directly accepts categorical features without needing manual encoding like one-hot or label encoding.\n",
        "\n",
        "This avoids the curse of dimensionality and sparseness issues common with one-hot encoding.\n",
        "\n",
        "2. Ordered Target Statistics (Ordered Boosting)\n",
        "CatBoost uses a smart technique called ordered target statistics or ordered boosting:\n",
        "\n",
        "It replaces categories with statistics computed on the training target (like the average target value for each category).\n",
        "\n",
        "To avoid target leakage, these statistics are computed in an online fashion, only using data from previous examples, preserving the proper training data distribution.\n",
        "\n",
        "3. Efficient Handling of High-Cardinality Categories\n",
        "Unlike naive encoding, CatBoost can efficiently handle categorical features with many unique values (high cardinality).\n",
        "\n",
        "It learns useful representations without exploding feature space.\n",
        "\n",
        "4. Reduced Overfitting on Categorical Data\n",
        "The ordered boosting approach reduces overfitting that can occur when naive target encoding is used.\n",
        "\n",
        "This leads to better generalization.\n",
        "\n",
        "5. Built-in Support for Missing Values in Categories\n",
        "Missing categorical data is handled gracefully without needing explicit imputation."
      ],
      "metadata": {
        "id": "Sxy68_Q51WzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9QRK5PV1BvR"
      },
      "outputs": [],
      "source": [
        "# 14.  Train an AdaBoost Classifier on a sample dataset and print model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost classifier with default parameters\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 15.  Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load dataset\n",
        "data = load_boston()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost regressor\n",
        "model = AdaBoostRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Absolute Error\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f\"AdaBoost Regressor Mean Absolute Error: {mae:.4f}\")\n"
      ],
      "metadata": {
        "id": "jgXgT9136b7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16.  Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feat_imp_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "print(feat_imp_df)\n",
        "\n",
        "# Optional: plot feature importance\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(feat_imp_df['Feature'], feat_imp_df['Importance'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Feature Importance in Gradient Boosting Classifier')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LvJrfmiT6jTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17.  Train a Gradient Boosting Regressor and evaluate using R-Squared Score\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Gradient Boosting Regressor R-squared Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "K1I-o0SJ6poS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18.  Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train Gradient Boosting\n",
        "gb_model.fit(X_train, y_train)\n",
        "gb_preds = gb_model.predict(X_test)\n",
        "gb_accuracy = accuracy_score(y_test, gb_preds)\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_preds = xgb_model.predict(X_test)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_preds)\n",
        "\n",
        "print(f\"Gradient Boosting Classifier Accuracy: {gb_accuracy:.4f}\")\n",
        "print(f\"XGBoost Classifier Accuracy: {xgb_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "LU0gtdYa6w6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19.  Train a CatBoost Classifier and evaluate using F1-Score\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize CatBoost Classifier (silent=True to suppress training output)\n",
        "model = CatBoostClassifier(random_state=42, silent=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate F1-Score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"CatBoost Classifier F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "Wq3dtj7562mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20.  Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Regressor\n",
        "model = XGBRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"XGBoost Regressor Mean Squared Error: {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "mgfzLKAc6_Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 21.  Train an AdaBoost Classifier and visualize feature importance\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Sort feature importances in descending order\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title(\"Feature Importances from AdaBoost Classifier\")\n",
        "plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
        "plt.xticks(range(len(importances)), feature_names[indices], rotation=90)\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7nzSuzSm7GsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22.  Train a Gradient Boosting Regressor and plot learning curves\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor with warm_start=True to track progress\n",
        "model = GradientBoostingRegressor(n_estimators=200, random_state=42)\n",
        "\n",
        "# Train model and record training and validation errors at each iteration\n",
        "train_errors = []\n",
        "val_errors = []\n",
        "\n",
        "for n_estimators in range(1, 201):\n",
        "    model.set_params(n_estimators=n_estimators)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_pred = model.predict(X_train)\n",
        "    val_pred = model.predict(X_val)\n",
        "\n",
        "    train_mse = mean_squared_error(y_train, train_pred)\n",
        "    val_mse = mean_squared_error(y_val, val_pred)\n",
        "\n",
        "    train_errors.append(train_mse)\n",
        "    val_errors.append(val_mse)\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(1, 201), train_errors, label='Training MSE')\n",
        "plt.plot(range(1, 201), val_errors, label='Validation MSE')\n",
        "plt.xlabel('Number of Trees (Estimators)')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Learning Curves for Gradient Boosting Regressor')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bnw36wj17POx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23.  Train an XGBoost Classifier and visualize feature importance\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train XGBoost Classifier\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10,8))\n",
        "plot_importance(model, max_num_features=15, height=0.8, importance_type='gain', xlabel='Gain', show_values=False)\n",
        "plt.title('Feature Importance - XGBoost Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1OM_uhjI7XQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24.  Train a CatBoost Classifier and plot the confusion matrix\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train CatBoost Classifier\n",
        "model = CatBoostClassifier(random_state=42, silent=True)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('CatBoost Classifier Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nmITiaqU7dvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25.  Train an AdaBoost Classifier with different numbers of estimators and compare accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different numbers of estimators to try\n",
        "estimators_list = [10, 50, 100, 200, 300]\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "for n_estimators in estimators_list:\n",
        "    model = AdaBoostClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Estimators: {n_estimators} - Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(estimators_list, accuracies, marker='o')\n",
        "plt.title('AdaBoost Accuracy vs Number of Estimators')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_H3lPTjE7nUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26.  Train a Gradient Boosting Classifier and visualize the ROC curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Gradient Boosting Classifier')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ftu2dKHO7t15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27.  Train an XGBoost Regressor and tune the learning rate using GridSearchCV\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Regressor\n",
        "xgb = XGBRegressor(random_state=42)\n",
        "\n",
        "# Define parameter grid to tune learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
        "\n",
        "# Run grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(f\"Best learning rate: {grid_search.best_params_['learning_rate']}\")\n",
        "print(f\"Best CV MSE: {-grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set with the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Test set Mean Squared Error: {test_mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "m9SFj4Hf71NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28.  Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=5000,\n",
        "    n_features=20,\n",
        "    n_classes=2,\n",
        "    weights=[0.9, 0.1],  # 90% of class 0, 10% of class 1 (minority)\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train CatBoost without class weights\n",
        "model_no_weights = CatBoostClassifier(random_state=42, silent=True)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "# Compute class weights manually\n",
        "# Weight for class i = total_samples / (num_classes * samples_in_class_i)\n",
        "class_counts = np.bincount(y_train)\n",
        "total = len(y_train)\n",
        "num_classes = len(class_counts)\n",
        "class_weights = {i: total / (num_classes * count) for i, count in enumerate(class_counts)}\n",
        "\n",
        "# Train CatBoost with class weights\n",
        "model_with_weights = CatBoostClassifier(class_weights=class_weights, random_state=42, silent=True)\n",
        "model_with_weights.fit(X_train, y_train)\n",
        "pred_with_weights = model_with_weights.predict(X_test)\n",
        "\n",
        "# Compare performance using classification report (includes precision, recall, F1)\n",
        "print(\"Without Class Weights:\")\n",
        "print(classification_report(y_test, pred_no_weights))\n",
        "\n",
        "print(\"With Class Weights:\")\n",
        "print(classification_report(y_test, pred_with_weights))\n"
      ],
      "metadata": {
        "id": "nn591ag-7_dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29.  Train an AdaBoost Classifier and analyze the effect of different learning rates\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different learning rates to try\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.5, 1]\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model = AdaBoostClassifier(learning_rate=lr, n_estimators=50, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Learning Rate: {lr} - Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(learning_rates, accuracies, marker='o')\n",
        "plt.title('AdaBoost Accuracy vs Learning Rate')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZP0Oq0v98Hwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30.  Train an XGBoost Classifier for multi-class classification and evaluate using log-loss.\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier for multi-class (use softprob for probabilities)\n",
        "model = XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    num_class=3,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for test set\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "# Calculate log-loss\n",
        "loss = log_loss(y_test, y_proba)\n",
        "print(f\"Multi-class Log Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "1uwZn_wG8Q6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}